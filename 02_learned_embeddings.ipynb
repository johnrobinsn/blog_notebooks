{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2da6afa2",
   "metadata": {},
   "source": [
    "# Part 2: Learned Positional Embeddings\n",
    "\n",
    "**Why Transformers Need Help Knowing Where** | *A Six-Part Series*\n",
    "\n",
    "<img src=\"https://www.storminthecastle.com/img/learned_pe.png\" width=\"256px\">\n",
    "\n",
    "---\n",
    "\n",
    "üìö **Series Navigation:**\n",
    "- Part 1: [The Position Problem & Sinusoidal PE](https://www.storminthecastle.com/posts/01_position_problem_sinusoidal/)\n",
    "- **Part 2: Learned Positional Embeddings** ‚Üê You are here\n",
    "- Part 3: RoPE (Rotary Position Embeddings) - Coming Soon\n",
    "- Part 4: ALiBi (Attention with Linear Biases) - Coming Soon\n",
    "- Part 5: PoPE (Polar Coordinate Embeddings) - Coming Soon\n",
    "- Part 6: Practitioner's Guide - Coming Soon\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In Part 1, we explored sinusoidal positional encodings ‚Äî a fixed, mathematical approach to encoding position. But what if we just... let the model figure it out?\n",
    "\n",
    "**Learned positional embeddings** take a radically simple approach: create a trainable lookup table where each position gets its own learnable vector. This is the approach used by **BERT**, **GPT-2**, and many other influential models.\n",
    "\n",
    "This notebook covers:\n",
    "1. How learned PE works (spoiler: it's just `nn.Embedding`)\n",
    "2. Implementation and visualization\n",
    "3. The critical **extrapolation limitation**\n",
    "4. When to choose learned vs. fixed encodings\n",
    "\n",
    "> üìÑ **Paper:** [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805) (Devlin et al., 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f8a81e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once)\n",
    "!pip install -q numpy matplotlib seaborn torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2815288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì All libraries imported successfully!\n",
      "PyTorch version: 2.9.1+cu128\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "# Set visualization styles\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['figure.figsize'] = [10, 6]\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"‚úì All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e2431f",
   "metadata": {},
   "source": [
    "## 1. The Simplest Approach: Just Learn It\n",
    "\n",
    "### Motivation\n",
    "\n",
    "Instead of designing a clever mathematical function to encode position, learned positional embeddings ask: **Why not let the model learn the best way to represent each position?**\n",
    "\n",
    "### ‚ö†Ô∏è Key Clarification: What's Given vs. What's Learned\n",
    "\n",
    "This is a common point of confusion, so let's be precise:\n",
    "\n",
    "| Component | Status | Explanation |\n",
    "|-----------|--------|-------------|\n",
    "| **Position index** (0, 1, 2, ...) | **Explicitly provided** | The model is *told* \"this token is at position 5\" ‚Äî it doesn't discover this from context |\n",
    "| **Embedding vector** for each position | **Learned via backpropagation** | The model learns *what values* best represent \"position 5\" |\n",
    "\n",
    "**The position index IS the ground truth signal.** When we process a sequence, we don't ask the model to figure out which position each token occupies ‚Äî we explicitly look up row 0 for position 0, row 5 for position 5, etc. This mapping is hardwired.\n",
    "\n",
    "**What's learned is HOW to represent each position** ‚Äî the actual d-dimensional vector of floats that gets added to the token embedding.\n",
    "\n",
    "> üîë **Analogy:** Think of a classroom seating chart. The seat numbers (positions 0, 1, 2...) are fixed and labeled on each desk. What's \"learned\" over time is the personality/reputation associated with each seat (\"the front row is for eager students\"). The model learns useful representations, but it's never confused about *which seat is which*.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. Create a lookup table (matrix) with one row per position, randomly initialized\n",
    "2. During training, use the **known position index** to look up the corresponding row\n",
    "3. Backpropagation updates the embedding values to minimize loss\n",
    "\n",
    "$$\\text{output}_i = \\text{token\\_embedding}_i + \\text{position\\_embedding}[i]$$\n",
    "\n",
    "Where `position_embedding[i]` means: \"look up row $i$ in the embedding table\" ‚Äî the index $i$ is known, the values in that row are learned.\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "Given:\n",
    "- Maximum sequence length $L$\n",
    "- Embedding dimension $d$\n",
    "\n",
    "The position embedding matrix $P \\in \\mathbb{R}^{L \\times d}$ is initialized randomly and optimized during training:\n",
    "\n",
    "$$P = \\begin{bmatrix} p_0 \\\\ p_1 \\\\ \\vdots \\\\ p_{L-1} \\end{bmatrix}$$\n",
    "\n",
    "For a token at position $i$:\n",
    "$$e_i^{\\text{final}} = e_i^{\\text{token}} + p_i$$\n",
    "\n",
    "### What This Is NOT\n",
    "\n",
    "To further clarify, learned positional embeddings are **not**:\n",
    "- Learning to infer position from surrounding context\n",
    "- Discovering position through attention patterns\n",
    "- Figuring out \"where am I?\" from the content\n",
    "\n",
    "Those would be fascinating (and much harder!) problems. Instead, this is simply: \"Given that I know this token is at position $i$, what's the best vector to represent that position?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c742785f",
   "metadata": {},
   "source": [
    "## 2. Implementation\n",
    "\n",
    "Let's implement learned positional embeddings as a PyTorch module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b389f4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 10, 64])\n",
      "Output shape: torch.Size([2, 10, 64])\n",
      "\n",
      "Position embedding table shape: torch.Size([512, 64])\n",
      "  ‚Üí 512 positions √ó 64 dimensions\n",
      "\n",
      "Trainable parameters: 32,768\n",
      "\n",
      "‚úì LearnedPositionalEmbedding module working!\n"
     ]
    }
   ],
   "source": [
    "class LearnedPositionalEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Learned positional embeddings as used in BERT and GPT-2.\n",
    "    \n",
    "    Each position (0, 1, 2, ..., max_len-1) has its own trainable \n",
    "    embedding vector of dimension d_model.\n",
    "    \n",
    "    Args:\n",
    "        d_model: Dimension of the embeddings\n",
    "        max_len: Maximum sequence length\n",
    "        dropout: Dropout probability\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, max_len: int = 512, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # The key component: a learnable embedding table\n",
    "        # Shape: (max_len, d_model)\n",
    "        self.position_embeddings = nn.Embedding(max_len, d_model)\n",
    "        \n",
    "        # Register position indices as a buffer (not a parameter)\n",
    "        # This avoids creating new tensors on every forward pass\n",
    "        self.register_buffer(\n",
    "            'position_ids', \n",
    "            torch.arange(max_len).unsqueeze(0)  # Shape: (1, max_len)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Add learned positional embeddings to input.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_len, d_model)\n",
    "        \n",
    "        Returns:\n",
    "            Tensor with positional embeddings added\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        \n",
    "        # Get position IDs for current sequence length\n",
    "        position_ids = self.position_ids[:, :seq_len]  # Shape: (1, seq_len)\n",
    "        \n",
    "        # Look up position embeddings\n",
    "        position_embeds = self.position_embeddings(position_ids)  # (1, seq_len, d_model)\n",
    "        \n",
    "        # Add to input (broadcasts across batch dimension)\n",
    "        x = x + position_embeds\n",
    "        \n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "# Test the module\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "d_model = 64\n",
    "\n",
    "learned_pe = LearnedPositionalEmbedding(d_model, max_len=512)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "output = learned_pe(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nPosition embedding table shape: {learned_pe.position_embeddings.weight.shape}\")\n",
    "print(f\"  ‚Üí {512} positions √ó {d_model} dimensions\")\n",
    "print(f\"\\nTrainable parameters: {learned_pe.position_embeddings.weight.numel():,}\")\n",
    "print(\"\\n‚úì LearnedPositionalEmbedding module working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65915700",
   "metadata": {},
   "source": [
    "## 3. What Do Learned Embeddings Look Like After Training?\n",
    "\n",
    "At initialization, learned positional embeddings are just random noise ‚Äî there's no meaningful structure. However, **after training on large corpora, learned embeddings typically develop smooth, structured patterns** that often resemble sinusoidal encodings.\n",
    "\n",
    "Research has shown that:\n",
    "- **Nearby positions develop similar embeddings** ‚Äî the model learns that position 5 should be more similar to position 6 than to position 100\n",
    "- **Smooth gradients emerge** ‚Äî embeddings change gradually across positions rather than randomly\n",
    "- **Task-specific patterns appear** ‚Äî unlike fixed sinusoidal PE, learned embeddings can adapt to the specific positional patterns useful for the training task\n",
    "\n",
    "> ÔøΩÔøΩ **Empirical finding:** When visualized as heatmaps, trained learned embeddings often show wave-like patterns similar to sinusoidal PE, suggesting the model \"rediscovers\" useful mathematical structure through gradient descent.\n",
    "\n",
    "This convergence to structured patterns provides some validation that sinusoidal encodings were a reasonable design choice ‚Äî but learned embeddings have the flexibility to deviate when beneficial for the task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c715b2",
   "metadata": {},
   "source": [
    "## 4. The Parameter Cost: Pros, Cons, and Trade-offs\n",
    "\n",
    "### Advantages\n",
    "\n",
    "| Aspect | Benefit |\n",
    "|--------|---------|\n",
    "| ‚úÖ **Flexibility** | Can adapt to task-specific positional patterns |\n",
    "| ‚úÖ **Simplicity** | Very easy to implement (just `nn.Embedding`) |\n",
    "| ‚úÖ **Proven** | Used successfully in BERT, GPT-2, GPT-3 |\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "| Aspect | Drawback |\n",
    "|--------|----------|\n",
    "| ‚ùå **Fixed Length** | Cannot handle sequences longer than `max_len` |\n",
    "| ‚ùå **Extrapolation** | Poor generalization to unseen positions |\n",
    "| ‚ùå **Parameters** | Adds $L \\times d$ trainable parameters |\n",
    "| ‚ùå **Data Hungry** | Needs enough data to learn good representations |\n",
    "\n",
    "### The Parameter Overhead\n",
    "\n",
    "Unlike sinusoidal PE (which has **zero** trainable parameters), learned PE adds significant overhead:\n",
    "\n",
    "$$\\text{PE Parameters} = \\text{max\\_len} \\times d_{\\text{model}}$$\n",
    "\n",
    "These parameters consume GPU memory and must be trained ‚Äî positions that appear rarely in training data may have poorly-learned representations. For long-context models (32k, 100k+ tokens), this becomes impractical: a 100k √ó 4096 table would add **400M parameters** just for positions. This is why modern long-context models use RoPE or ALiBi instead.\n",
    "\n",
    "### Parameter Count Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97082b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LEARNED PE PARAMETER COUNTS\n",
      "============================================================\n",
      "\n",
      "Model           Max Len    d_model    PE Params      \n",
      "------------------------------------------------------------\n",
      "BERT-base       512        768        393,216\n",
      "GPT-2 small     1024       768        786,432\n",
      "GPT-2 medium    1024       1024       1,048,576\n",
      "GPT-2 large     1024       1280       1,310,720\n",
      "------------------------------------------------------------\n",
      "\n",
      "Note: Sinusoidal PE has 0 trainable parameters!\n",
      "      These PE parameters are ~0.4-1.3M per model.\n"
     ]
    }
   ],
   "source": [
    "# Parameter count for different configurations\n",
    "configs = [\n",
    "    (\"BERT-base\", 512, 768),\n",
    "    (\"GPT-2 small\", 1024, 768),\n",
    "    (\"GPT-2 medium\", 1024, 1024),\n",
    "    (\"GPT-2 large\", 1024, 1280),\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LEARNED PE PARAMETER COUNTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n{'Model':<15} {'Max Len':<10} {'d_model':<10} {'PE Params':<15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for name, max_len, d_model in configs:\n",
    "    params = max_len * d_model\n",
    "    print(f\"{name:<15} {max_len:<10} {d_model:<10} {params:,}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"\\nNote: Sinusoidal PE has 0 trainable parameters!\")\n",
    "print(\"      These PE parameters are ~0.4-1.3M per model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92527dc7",
   "metadata": {},
   "source": [
    "## 5. The Extrapolation Problem\n",
    "\n",
    "The most critical limitation of learned positional embeddings: **they cannot handle sequences longer than `max_len`**.\n",
    "\n",
    "This is because the embedding table has a fixed size. Position 512 in a model trained with `max_len=512` simply doesn't exist ‚Äî there's no vector to look up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18913f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LEARNED PE: EXTRAPOLATION LIMITATION\n",
      "======================================================================\n",
      "\n",
      "Module created with max_len=100\n",
      "\n",
      "Sequence Length      Status                        \n",
      "----------------------------------------------------------------------\n",
      "50                   ‚úì Works\n",
      "100                  ‚úì Works\n",
      "101                  ‚úó FAILS (RuntimeError)\n",
      "\n",
      "======================================================================\n",
      "CRITICAL: Sequences longer than max_len cause errors!\n",
      "======================================================================\n",
      "\n",
      "This is why modern LLMs (2023+) prefer RoPE or ALiBi:\n",
      "‚Ä¢ RoPE: Computes rotation on-the-fly, no position limit\n",
      "‚Ä¢ ALiBi: Computes bias on-the-fly, no position limit\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate the extrapolation limitation\n",
    "print(\"=\" * 70)\n",
    "print(\"LEARNED PE: EXTRAPOLATION LIMITATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create module with max_len=100\n",
    "learned_pe_short = LearnedPositionalEmbedding(d_model=64, max_len=100)\n",
    "\n",
    "test_lengths = [50, 100, 101]\n",
    "\n",
    "print(f\"\\nModule created with max_len=100\")\n",
    "print(f\"\\n{'Sequence Length':<20} {'Status':<30}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for length in test_lengths:\n",
    "    try:\n",
    "        x_test = torch.randn(1, length, 64)\n",
    "        _ = learned_pe_short(x_test)\n",
    "        print(f\"{length:<20} ‚úì Works\")\n",
    "    except Exception as e:\n",
    "        error_type = type(e).__name__\n",
    "        print(f\"{length:<20} ‚úó FAILS ({error_type})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CRITICAL: Sequences longer than max_len cause errors!\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nThis is why modern LLMs (2023+) prefer RoPE or ALiBi:\")\n",
    "print(\"‚Ä¢ RoPE: Computes rotation on-the-fly, no position limit\")\n",
    "print(\"‚Ä¢ ALiBi: Computes bias on-the-fly, no position limit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682b7407",
   "metadata": {},
   "source": [
    "## 6. Should You Use Learned PE? (Probably Not for New Projects)\n",
    "\n",
    "### The Honest Assessment\n",
    "\n",
    "For **new projects in 2025+**, there's little reason to choose learned positional embeddings over alternatives like RoPE:\n",
    "\n",
    "| Approach | Parameters | Extrapolation | Complexity |\n",
    "|----------|------------|---------------|------------|\n",
    "| **Learned PE** | $L \\times d$ | ‚ùå Hard limit at `max_len` | Simple |\n",
    "| **Sinusoidal PE** | 0 | ‚ö†Ô∏è Degrades beyond training | Simple |\n",
    "| **RoPE** | 0 | ‚úÖ Generalizes well | Moderate |\n",
    "| **ALiBi** | 0 | ‚úÖ Generalizes well | Simple |\n",
    "\n",
    "RoPE has become the de facto standard for modern LLMs (LLaMA, Mistral, GPT-4) because it offers:\n",
    "- **Zero additional parameters**\n",
    "- **No sequence length limit**\n",
    "- **Strong empirical performance**\n",
    "\n",
    "### When Learned PE Still Makes Sense\n",
    "\n",
    "1. **Fine-tuning existing models**: If you're using BERT, GPT-2, or other pre-trained models that already use learned PE, you'd continue with that architecture\n",
    "\n",
    "2. **Fixed-length classification**: Tasks like sentiment analysis where you'll never exceed `max_len` and don't need extrapolation\n",
    "\n",
    "3. **Educational purposes**: `nn.Embedding` is trivially simple to understand compared to RoPE's rotation matrices\n",
    "\n",
    "4. **Reproducing published results**: When replicating papers that used learned PE\n",
    "\n",
    "### The Bottom Line\n",
    "\n",
    "> üéØ **Recommendation:** Unless you have a specific reason to use learned PE (backwards compatibility, reproducing prior work), **default to RoPE for new projects**. It's what the field has converged on, and the extrapolation problem alone makes learned PE a risky choice for any application where sequence lengths might grow.\n",
    "\n",
    "We include learned PE in this series because it's historically important (BERT, GPT-2) and conceptually illuminating ‚Äî but it's largely a legacy approach at this point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7dd129",
   "metadata": {},
   "source": [
    "## 7. Complete BERT-Style Embedding Layer\n",
    "\n",
    "Let's build a complete embedding layer that combines token embeddings with learned positional embeddings, exactly as BERT does it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdc2a1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 128])\n",
      "Output shape: torch.Size([2, 128, 768])\n",
      "\n",
      "Total embedding parameters:\n",
      "  Token embeddings: 23,040,000\n",
      "  Position embeddings: 393,216\n",
      "  Total: 23,433,216\n",
      "\n",
      "‚úì BERT-style embedding layer working!\n"
     ]
    }
   ],
   "source": [
    "class BERTStyleEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete BERT-style embedding layer with:\n",
    "    - Token embeddings\n",
    "    - Learned positional embeddings\n",
    "    - (Optional) Segment/type embeddings\n",
    "    - Layer normalization\n",
    "    - Dropout\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_size: int, \n",
    "        d_model: int, \n",
    "        max_len: int = 512,\n",
    "        dropout: float = 0.1,\n",
    "        use_segment_embeddings: bool = False,\n",
    "        n_segments: int = 2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Token embeddings\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # Position embeddings\n",
    "        self.position_embeddings = nn.Embedding(max_len, d_model)\n",
    "        \n",
    "        # Segment embeddings (optional, for sentence pair tasks)\n",
    "        self.use_segment_embeddings = use_segment_embeddings\n",
    "        if use_segment_embeddings:\n",
    "            self.segment_embeddings = nn.Embedding(n_segments, d_model)\n",
    "        \n",
    "        # Layer norm and dropout\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Position IDs buffer\n",
    "        self.register_buffer(\n",
    "            'position_ids',\n",
    "            torch.arange(max_len).unsqueeze(0)\n",
    "        )\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids: torch.Tensor,\n",
    "        segment_ids: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids: Token indices, shape (batch, seq_len)\n",
    "            segment_ids: Segment indices, shape (batch, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            Combined embeddings, shape (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        seq_len = input_ids.size(1)\n",
    "        \n",
    "        # Get embeddings\n",
    "        token_emb = self.token_embeddings(input_ids)\n",
    "        position_emb = self.position_embeddings(self.position_ids[:, :seq_len])\n",
    "        \n",
    "        # Combine\n",
    "        embeddings = token_emb + position_emb\n",
    "        \n",
    "        if self.use_segment_embeddings and segment_ids is not None:\n",
    "            segment_emb = self.segment_embeddings(segment_ids)\n",
    "            embeddings = embeddings + segment_emb\n",
    "        \n",
    "        # Normalize and dropout\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "\n",
    "# Test\n",
    "bert_embedding = BERTStyleEmbedding(\n",
    "    vocab_size=30000,\n",
    "    d_model=768,\n",
    "    max_len=512\n",
    ")\n",
    "\n",
    "# Simulate input token IDs\n",
    "input_ids = torch.randint(0, 30000, (2, 128))\n",
    "output = bert_embedding(input_ids)\n",
    "\n",
    "print(f\"Input shape: {input_ids.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nTotal embedding parameters:\")\n",
    "print(f\"  Token embeddings: {30000 * 768:,}\")\n",
    "print(f\"  Position embeddings: {512 * 768:,}\")\n",
    "print(f\"  Total: {30000 * 768 + 512 * 768:,}\")\n",
    "print(\"\\n‚úì BERT-style embedding layer working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c59ee4",
   "metadata": {},
   "source": [
    "## 8. Summary and Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Learned PE is Simple**: Just an `nn.Embedding` table\n",
    "   - Position $i$ ‚Üí learnable vector $p_i$\n",
    "   - Added to token embedding\n",
    "\n",
    "2. **Trade-offs**:\n",
    "   | Pro | Con |\n",
    "   |-----|-----|\n",
    "   | Flexible, adapts to task | Fixed max length |\n",
    "   | Simple implementation | Cannot extrapolate |\n",
    "   | Proven in BERT, GPT-2 | More parameters |\n",
    "\n",
    "3. **The Extrapolation Problem**:\n",
    "   - Cannot handle `seq_len > max_len`\n",
    "   - This motivated RoPE and ALiBi\n",
    "\n",
    "4. **Use When**:\n",
    "   - Fixed-length tasks (classification, NER)\n",
    "   - Following BERT/GPT-2 architecture\n",
    "   - Have large training data\n",
    "\n",
    "### Coming Up Next\n",
    "\n",
    "**Part 3: RoPE (Rotary Position Embeddings)**\n",
    "- The rotation trick that enables unlimited sequence length\n",
    "- Why LLaMA, Mistral, and GPT-4 use RoPE\n",
    "- Elegant math: complex numbers and rotation matrices\n",
    "\n",
    "---\n",
    "\n",
    "### References\n",
    "\n",
    "1. **Devlin, J., et al.** (2018). \"BERT: Pre-training of Deep Bidirectional Transformers.\"\n",
    "   - [arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)\n",
    "   \n",
    "2. **Radford, A., et al.** (2019). \"Language Models are Unsupervised Multitask Learners.\" (GPT-2)\n",
    "   - Learned positional embeddings with max_len=1024\n",
    "\n",
    "---\n",
    "\n",
    "*Last updated: January 2026*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  },
  "blog": {
   "title": "Learning Position ‚Äî Trainable Embeddings from BERT to GPT",
   "description": "Learning Position ‚Äî Trainable Embeddings from BERT to GPT",
   "date": "2026-01-06",
   "tags": [
    "positional encoding",
    "learned embeddings",
    "transformers"
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}